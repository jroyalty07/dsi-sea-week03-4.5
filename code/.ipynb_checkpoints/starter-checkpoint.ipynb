{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "___Hello? Can anyone hear me?___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<img src=\"https://media.giphy.com/media/xaMg6NGwH2fFS/giphy.gif\" alt=\"Title\" style=\"border: 5px solid #000000; padding: 10px; width: 250px; height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## There. Is that better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<img src=\"http://bestanimations.com/Sci-Fi/Aliens/little-grey-extraterrestial-aliens-animated-gif-image-10.gif\" alt=\"Title\" style=\"border: 5px solid #000000; padding: 10px; width: 250px; height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Good... Hello fellow person. My name is Hugh Man. I am also a person like you! Here is a picture of me at a normal human party with my normal human friends!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<img src=\"http://www.promvenues.com/blogs/media/Crazy%20Prom%20Themes%20for%20Teenagers%20to%20Spice%20up%20Their%20Party_142.jpg\" alt=\"Title\" style=\"border: 5px solid #000000; padding: 10px; width: 250px; height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Look how much fun we are having drinking everyday human liquids!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Anyways, I have sent this communication to ask you a few questions about normal human activities, that I also do. My human mother is very sick and doesn't get out from her two story Dutch Colonial condo as often as she would like. Here is a picture of her from yesterday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<img src=\"http://previews.123rf.com/images/szefei/szefei1302/szefei130200062/18061194-Happy-mother-s-day-concept-Asian-senior-mother-showing-a-gift-and-carnation-flowers-at-home--Stock-Photo.jpg\" alt=\"Title\" style=\"border: 5px solid #000000; padding: 10px; width: 250px; height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## She was wondering about the state of technological / military advancement on your human planet. She is very fearful of planets with enough technology to shoot down class C starships from space. Can you help put her fears to rest? My assistant will beam the details to your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "___Beep___\n",
    "### Assistant here! I have updated your notebook with the proper protocols. Please follow the instructions provided.\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "* Access the 'conspiracy' site (https://www.reddit.com/r/conspiracy.json). You should get the first 25 entries from the subreddit right away.\n",
    "* Make sure to keep track of the entry position, title, and upvotes for each entry in the subreddit\n",
    "* Crawl through each url. Extract all the text from each paragraph (they have a <**p**> tag). If there is no url, skip this step.\n",
    "* Crawl through each url. Extract all the links from each link tag (they have an <**a**> tag). If there is no url, skip this step.\n",
    "\n",
    "*** Bonus ***\n",
    "\n",
    "* Crawl through each of the comments. Extract all the text from the body keys of each comment\n",
    "\n",
    "#### Tips\n",
    "\n",
    "* Each entry on the site contains a key called \"permalink\" add it to your base url to navigate to that link's comments\n",
    "* Each entry on the site contains a key called \"url\". Use this link to navigate to the page described in the post\n",
    "\n",
    "<br>\n",
    "** Base Url => https://www.reddit.com **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jroyalty/anaconda/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /Users/jroyalty/anaconda/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "base = 'https://www.reddit.com'\n",
    "soup = BeautifulSoup(urllib2.urlopen('https://www.reddit.com/r/conspiracy.json').read())\n",
    "parsed_json = json.loads(soup.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jroyalty/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:19: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Project Veritas: Clinton Campaign and DNC Inci...</td>\n",
       "      <td>https://www.youtube.com/watch?v=5IuJGHuIkzY</td>\n",
       "      <td>/r/conspiracy/comments/57y4fr/project_veritas_...</td>\n",
       "      <td>4070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rigging the Election - Video II: Mass Voter Fraud</td>\n",
       "      <td>https://www.youtube.com/watch?v=hDc8PVCvfKs</td>\n",
       "      <td>/r/conspiracy/comments/5856sv/rigging_the_elec...</td>\n",
       "      <td>2811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jesus!! Hillary's $12M donation to the Clinton...</td>\n",
       "      <td>http://i.imgur.com/pbW8GYp.jpg</td>\n",
       "      <td>/r/conspiracy/comments/58kqkm/jesus_hillarys_1...</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pentagon: Hillary Clinton Should Be Arrested F...</td>\n",
       "      <td>http://truepundit.com/pentagon-hillary-clinton...</td>\n",
       "      <td>/r/conspiracy/comments/58hgh6/pentagon_hillary...</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The government has known since 1974 that eatin...</td>\n",
       "      <td>https://www.youtube.com/watch?v=NicBNNGZijQ</td>\n",
       "      <td>/r/conspiracy/comments/58kss8/the_government_h...</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Project Veritas: Clinton Campaign and DNC Inci...   \n",
       "1  Rigging the Election - Video II: Mass Voter Fraud   \n",
       "2  Jesus!! Hillary's $12M donation to the Clinton...   \n",
       "3  Pentagon: Hillary Clinton Should Be Arrested F...   \n",
       "4  The government has known since 1974 that eatin...   \n",
       "\n",
       "                                                 url  \\\n",
       "0        https://www.youtube.com/watch?v=5IuJGHuIkzY   \n",
       "1        https://www.youtube.com/watch?v=hDc8PVCvfKs   \n",
       "2                     http://i.imgur.com/pbW8GYp.jpg   \n",
       "3  http://truepundit.com/pentagon-hillary-clinton...   \n",
       "4        https://www.youtube.com/watch?v=NicBNNGZijQ   \n",
       "\n",
       "                                           permalink   ups  \n",
       "0  /r/conspiracy/comments/57y4fr/project_veritas_...  4070  \n",
       "1  /r/conspiracy/comments/5856sv/rigging_the_elec...  2811  \n",
       "2  /r/conspiracy/comments/58kqkm/jesus_hillarys_1...   824  \n",
       "3  /r/conspiracy/comments/58hgh6/pentagon_hillary...  1931  \n",
       "4  /r/conspiracy/comments/58kss8/the_government_h...   164  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = parsed_json['data']['children']\n",
    "df = pd.DataFrame()\n",
    "perma = []\n",
    "ups = []\n",
    "title = []\n",
    "url = []\n",
    "\n",
    "for x in range(len(dict)):\n",
    "    perma += [dict[x]['data']['permalink']]\n",
    "    ups += [dict[x]['data']['ups']]\n",
    "    title += [dict[x]['data']['title']]\n",
    "    url += [dict[x]['data']['url']]\n",
    "\n",
    "df['title'] = title\n",
    "df['url'] = url\n",
    "df['permalink'] = perma\n",
    "df['ups'] = ups\n",
    "\n",
    "df.sort('ups',ascending = False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 403: Forbidden\n",
      "HTTP Error 429: Too Many Requests\n",
      "<urlopen error [Errno 60] Operation timed out>\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n"
     ]
    }
   ],
   "source": [
    "\n",
    "paragraphs = {}\n",
    "links = {}\n",
    "\n",
    "order = 0\n",
    "\n",
    "# for each url\n",
    "for x in df['url']:\n",
    "    try:\n",
    "        soup = BeautifulSoup(urllib2.urlopen(x).read(), \"lxml\")\n",
    "        \n",
    "        paragraphs1 = []\n",
    "        links1 = []\n",
    "        #for each <p>\n",
    "        for idx, stuff in enumerate(soup.find_all('p')):\n",
    "            #print \"<=== p ===>\", idx\n",
    "            para = stuff.text.strip()\n",
    "            paragraphs1 += [stuff.text.strip()]\n",
    "        paragraphs[order] = paragraphs1\n",
    "        \n",
    "        # for each <a>\n",
    "        for idx, stuff in enumerate(soup.find_all('a',href=True)):\n",
    "            #print \"<=== a ===>\", idx\n",
    "            links1 += [stuff['href']]\n",
    "        links[order] = links1\n",
    "\n",
    "    except Exception as e:\n",
    "        print e\n",
    "        paragraphs[order] = []\n",
    "        links[order] = []\n",
    "    order += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Cleaning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Look through each of your words from your paragraph collecting. Remove as many of the nonsense values as you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "rejects = ['the','and','to','a','this','is','a','on','has','new','in', \n",
    "           'it', 'have', 'was','but','i','about','be','not','if','at',\n",
    "           'an','of','that','by','from','as','with','for','are','were',\n",
    "           'or','loading...','working...','playlists...', '','you','h',\n",
    "           't','he', 'loading' #cause loading annoys me\n",
    "          ]\n",
    "\n",
    "rejects += [str(x) for x in range(10)]\n",
    "\n",
    "def replaceText(stringy):\n",
    "    new_stringy = re.sub(\"[^0-9a-z]\", \"\",stringy)\n",
    "    if not new_stringy:\n",
    "        return ''\n",
    "    return new_stringy\n",
    "\n",
    "\n",
    "for key in paragraphs:\n",
    "    temp = []\n",
    "    for element in paragraphs[key]:\n",
    "        temp += element.split(' ')\n",
    "    paragraphs[key] = [replaceText(x.lower().replace('.','')) for x in temp]\n",
    "    paragraphs[key] = [x for x in paragraphs[key] if not x in rejects]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Now look at the links that you collected. Everything that doesn't begin with an http:// or an https:// is a direct link 99.9% of the time. Remove all links that don't begin with http:// or https://."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# breaks after first run\n",
    "# links = save_links\n",
    "\n",
    "# temp = []\n",
    "# for x in links: \n",
    "#     for element in links[x]:\n",
    "#         if 'http://' in element.encode('ascii','ignore') or 'https://' in element.encode('ascii','ignore'):\n",
    "#             temp += [element]\n",
    "#         links[x] = [temp]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Munging (aka digging into the data)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Every thing looks good buddy. Let's total up and print the top 10 words found in each of the urls with the subreddit's title above. Be sure to note any reference to class C starships in your report. O, and don't forget to remove the unnecessary words from the total. I don't think Supreme Commander Hugh Man would like to see words like 'the' and 'and' in his report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinton           4\n",
      "campaign          3\n",
      "dnc               3\n",
      "operative         2\n",
      "working           2\n",
      "video             2\n",
      "documents         1\n",
      "friggin           1\n",
      "communications    1\n",
      "between           1\n",
      "dtype: int64\n",
      "been         2\n",
      "video        2\n",
      "working      2\n",
      "view         1\n",
      "here         1\n",
      "playlists    1\n",
      "fraud        1\n",
      "second       1\n",
      "hillary      1\n",
      "okeefes      1\n",
      "dtype: int64\n",
      "Series([], dtype: int64)\n",
      "Series([], dtype: int64)\n",
      "actually     7\n",
      "smokers      5\n",
      "use          5\n",
      "weed         5\n",
      "less         4\n",
      "cannabis     4\n",
      "health       3\n",
      "copyright    3\n",
      "might        3\n",
      "working      3\n",
      "dtype: int64\n",
      "wikileaks       13\n",
      "timkaine        10\n",
      "tweet            7\n",
      "donnabrazile     7\n",
      "your             6\n",
      "kaydeeking       6\n",
      "more             5\n",
      "danethepain1     4\n",
      "location         4\n",
      "me               4\n",
      "dtype: int64\n",
      "children    72\n",
      "points      70\n",
      "hours       68\n",
      "ago0        40\n",
      "vote        34\n",
      "party       27\n",
      "point2      26\n",
      "points1     23\n",
      "points3     23\n",
      "they        22\n",
      "dtype: int64\n",
      "Series([], dtype: int64)\n",
      "she        14\n",
      "her         9\n",
      "clinton     8\n",
      "hillary     8\n",
      "lauer       7\n",
      "all         6\n",
      "nbc         6\n",
      "matt        5\n",
      "had         4\n",
      "out         4\n",
      "dtype: int64\n",
      "wikileaks          20\n",
      "we                  8\n",
      "wdfx2eu7            7\n",
      "your                7\n",
      "tweet               7\n",
      "tlkconservative     6\n",
      "more                6\n",
      "when                5\n",
      "up                  5\n",
      "they                4\n",
      "dtype: int64\n",
      "Series([], dtype: int64)\n",
      "focus           3\n",
      "debate          2\n",
      "vice            2\n",
      "presidential    2\n",
      "working         2\n",
      "undecided       2\n",
      "accused         2\n",
      "group           2\n",
      "oct             1\n",
      "manipulating    1\n",
      "dtype: int64\n",
      "Series([], dtype: int64)\n",
      "Series([], dtype: int64)\n",
      "wikileaks         15\n",
      "your               8\n",
      "tweet              7\n",
      "justinpulitzer     7\n",
      "more               6\n",
      "anthonybrown       5\n",
      "any                5\n",
      "michaelregular     5\n",
      "emails             4\n",
      "location           4\n",
      "dtype: int64\n",
      "Series([], dtype: int64)\n",
      "Series([], dtype: int64)\n",
      "Series([], dtype: int64)\n",
      "trump      26\n",
      "creamer    22\n",
      "your       21\n",
      "will       16\n",
      "his        16\n",
      "us         15\n",
      "vote       14\n",
      "people     14\n",
      "one        13\n",
      "when       13\n",
      "dtype: int64\n",
      "money         11\n",
      "sprott        10\n",
      "its            7\n",
      "any            6\n",
      "people         6\n",
      "act            6\n",
      "they           6\n",
      "ltd            6\n",
      "their          5\n",
      "government     5\n",
      "dtype: int64\n",
      "Series([], dtype: int64)\n",
      "wikileaks          20\n",
      "hillaryclinton      8\n",
      "tweet               7\n",
      "your                6\n",
      "kevjames91          6\n",
      "more                5\n",
      "she                 5\n",
      "there               5\n",
      "podestaemails13     4\n",
      "wrong               4\n",
      "dtype: int64\n",
      "grant503    13\n",
      "tweet        9\n",
      "your         6\n",
      "more         5\n",
      "what         5\n",
      "location     4\n",
      "can          4\n",
      "get          4\n",
      "they         4\n",
      "when         3\n",
      "dtype: int64\n",
      "hours       13\n",
      "children    13\n",
      "points      13\n",
      "ago0        11\n",
      "fbi          7\n",
      "your         5\n",
      "think        5\n",
      "points3      5\n",
      "do           4\n",
      "reddit       4\n",
      "dtype: int64\n",
      "they            23\n",
      "intelligence    23\n",
      "agencies        22\n",
      "all             18\n",
      "them            16\n",
      "we              16\n",
      "would           15\n",
      "who             14\n",
      "one             14\n",
      "know            14\n",
      "dtype: int64\n",
      "Series([], dtype: int64)\n",
      "isis          5\n",
      "lama          4\n",
      "dalai         4\n",
      "email         3\n",
      "cohesive      2\n",
      "getting       2\n",
      "peace         2\n",
      "terrorists    2\n",
      "believes      2\n",
      "putin         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# convert to series for easier counting.\n",
    "for key in paragraphs:\n",
    "    paragraphs[key] = pd.Series(paragraphs[key])\n",
    "    print paragraphs[key].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Those totals seem pretty small to me. I don't think Mr. Man's mother would approve of those results. Let's compile the words from the 10 entries with the highest upvotes. No need to print out the titles again. Just the top 10 words should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP TEN WORDS OF TOP TEN UPVOTES\n",
      "\n",
      "\n",
      "clinton           4\n",
      "campaign          3\n",
      "dnc               3\n",
      "operative         2\n",
      "working           2\n",
      "video             2\n",
      "documents         1\n",
      "friggin           1\n",
      "communications    1\n",
      "between           1\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "been         2\n",
      "video        2\n",
      "working      2\n",
      "view         1\n",
      "here         1\n",
      "playlists    1\n",
      "fraud        1\n",
      "second       1\n",
      "hillary      1\n",
      "okeefes      1\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Series([], dtype: int64)\n",
      "\n",
      "\n",
      "Series([], dtype: int64)\n",
      "\n",
      "\n",
      "actually     7\n",
      "smokers      5\n",
      "use          5\n",
      "weed         5\n",
      "less         4\n",
      "cannabis     4\n",
      "health       3\n",
      "copyright    3\n",
      "might        3\n",
      "working      3\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "wikileaks       13\n",
      "timkaine        10\n",
      "tweet            7\n",
      "donnabrazile     7\n",
      "your             6\n",
      "kaydeeking       6\n",
      "more             5\n",
      "danethepain1     4\n",
      "location         4\n",
      "me               4\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "children    72\n",
      "points      70\n",
      "hours       68\n",
      "ago0        40\n",
      "vote        34\n",
      "party       27\n",
      "point2      26\n",
      "points1     23\n",
      "points3     23\n",
      "they        22\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Series([], dtype: int64)\n",
      "\n",
      "\n",
      "she        14\n",
      "her         9\n",
      "clinton     8\n",
      "hillary     8\n",
      "lauer       7\n",
      "all         6\n",
      "nbc         6\n",
      "matt        5\n",
      "had         4\n",
      "out         4\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "wikileaks          20\n",
      "we                  8\n",
      "wdfx2eu7            7\n",
      "your                7\n",
      "tweet               7\n",
      "tlkconservative     6\n",
      "more                6\n",
      "when                5\n",
      "up                  5\n",
      "they                4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print \"TOP TEN WORDS OF TOP TEN UPVOTES\"\n",
    "for key in range(10):\n",
    "    paragraphs[key] = pd.Series(paragraphs[key])\n",
    "    print \"\\n\"\n",
    "    print paragraphs[key].value_counts()[:10]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Let's see how far this site's human tentacles spread! Print out a count of the top 10 base urls from your combined scrubbed links. \n",
    "\n",
    "___Remember a base url is everything after the first 'http://' or 'https://' and before the first '/' in a url. For example: the base url for https://www.google.com/intl/en/policies/privacy/ is www.google.com___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https:', '', 'www.google.com', 'get', 'videoqualityreport', '?v=5IuJGHuIkzY'], ['https:', '', 'accounts.google.com', 'ServiceLogin?passive=true&hl=en&uilel=3&service=youtube&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Fnext%3D%252Fwatch%253Fv%253D5IuJGHuIkzY%26action_handle_signin%3Dtrue%26hl%3Den%26app%3Ddesktop%26feature%3D__FEATURE__'], ['https:', '', 'accounts.google.com', 'ServiceLogin?passive=true&hl=en&uilel=3&service=youtube&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Fnext%3D%252Fwatch%253Fv%253D5IuJGHuIkzY%26action_handle_signin%3Dtrue%26hl%3Den%26app%3Ddesktop%26feature%3D__FEATURE__'], ['https:', '', 'accounts.google.com', 'ServiceLogin?passive=true&hl=en&uilel=3&service=youtube&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Fnext%3D%252Fwatch%253Fv%253D5IuJGHuIkzY%26action_handle_signin%3Dtrue%26hl%3Den%26app%3Ddesktop%26feature%3D__FEATURE__'], ['https:', '', 'accounts.google.com', 'ServiceLogin?passive=true&hl=en&uilel=3&service=youtube&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Fnext%3D%252Fwatch%253Fv%253D5IuJGHuIkzY%26action_handle_signin%3Dtrue%26hl%3Den%26app%3Ddesktop%26feature%3D__FEATURE__'], ['http:', '', 'projectveritasaction.com', ''], ['https:', '', 'www.facebook.com', 'ProjectVerit'], ['https:', '', 'twitter.com', '@Pveritas_Action'], ['https:', '', 'plus.google.com', '+youtube'], ['https:', '', 'www.google.com', 'intl', 'en', 'policies', 'privacy', '']]\n"
     ]
    }
   ],
   "source": [
    "# Split by '/', separates the base URL out to grab.\n",
    "temp = []\n",
    "for key in links:\n",
    "    for x in links[key]:\n",
    "        for y in x:\n",
    "            temp += [y.split('/')]\n",
    "print temp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP TEN BASE URLS!\n",
      "www.reddit.com          8000\n",
      "www.sprottmoney.com     1328\n",
      "accounts.google.com      400\n",
      "twitter.com              352\n",
      "portal.ransquawk.com     256\n",
      "www.benzinga.com         256\n",
      "www.thefly.com           256\n",
      "www.zerohedge.com        240\n",
      "support.twitter.com      160\n",
      "www.google.com           160\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_links = []\n",
    "for lst in temp:\n",
    "    for y in lst:\n",
    "        #only grab strings where first 4 chars are 'www.'\n",
    "        if 'www.' in y[:4] or \".com\" in y[len(y) - 5:]:\n",
    "            total_links += [y]\n",
    "\n",
    "\n",
    "total_links = pd.Series(total_links)\n",
    "print \"TOP TEN BASE URLS!\"\n",
    "print total_links.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# These are serious bonus questions. \n",
    "<br>\n",
    "** Not for the faint of heart! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Assistant back! Good work human friend. Let's total up and print the top 10 words found in all comment sections. In addition to starships, make sure there is no mention of aliens or lizard people. We don't want to scare Mr. Man's mother with ideas that obviously aren't true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Oops! Looks like we made a mistake. The word each and both is inconveniently close in Zar... I mean English. Please find the top 10 combined word totals from the comments and urls for ___EACH___ entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Good work friend! We can now send this data back to the human lab for analysis. Until next time guy. This is Hugh Man out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<img src=\"https://media.tenor.co/images/c44fc07f543327c1073653ce8e99c17e/raw\" alt=\"Title\" style=\"border: 5px solid #000000; padding: 10px; width: 250px; height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<img src=\"https://media.giphy.com/media/xaMg6NGwH2fFS/giphy.gif\" alt=\"Title\" style=\"border: 5px solid #000000; padding: 10px; width: 250px; height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
